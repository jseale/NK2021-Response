{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0eafc5ce",
   "metadata": {},
   "source": [
    "# Word embedding-based Homograph Disambigation Logistic Regression \n",
    "\n",
    "### Model\n",
    "Multinomial (one model per class) logistic regression (LR) for homograph disambiguation (HD).\n",
    "\n",
    "#### LR Features\n",
    "The feature for each homograph pronunciation label is a BERT token embedding. Each embedding is taken from the token embeddings for a sentence containing the homograph. \n",
    "\n",
    "### Data\n",
    "[Wikipedia Homograph Data (WHD)](https://github.com/google-research-datasets/WikipediaHomographData); see:\n",
    "Gorman, K., Mazovetskiy, G., and Nikolaev, V. (2018). [Improving homograph disambiguation with machine learning.](https://aclanthology.org/L18-1215/) In Proceedings of the Eleventh International Conference on Language Resources and Evaluation, pages 1349-1352. Miyazaki, Japan.\n",
    "\n",
    "### Context\n",
    " Nicolis and Klimkov (2021; [NK2021](https://www.researchgate.net/profile/Marco-Nicolis-2/publication/354151448_Homograph_disambiguation_with_contextual_word_embeddings_for_TTS_systems/links/613619910360302a0083e34b/Homograph-disambiguation-with-contextual-word-embeddings-for-TTS-systems.pdf)) claim SOTA results with word-embedding-featured HD LR. However, ~%40 of the classes (homograph pronunciations/wordids) in the WHD test set are represented by either one instance, or are _not_ represented in the WHD test set used by NK2021. Over the entire WHD, 17 of the homographs have only 1 pronunciation class. NK2021 take 'conglomerate' out from the data, as one pronunciation in the test set is not present in the training set. They use the rest of the WHD data as is, which possibly calls into question their results. Would the model(s) perform as well with a more robust test set and with each homograph having at least two pronunciations from which to select?\n",
    "\n",
    "### Purpose\n",
    "The HD LR in this notebook is developed to replicate experimentation found in NK2021. \n",
    "\n",
    "### Use\n",
    "1. Compare metrics obtained with NK2021-replicated HD LR  using the WHD to a data set that provides better class coverage.\n",
    "2. Compare metrics obtained in #1 to those obtained with multi-class token classifier developed in [Seale (2021)](https://academicworks.cuny.edu/cgi/viewcontent.cgi?article=5591&context=gc_etds).\n",
    "3. Determine if SOTA claims using HD LR still hold given data issues, and when compared to multi-class neural nets."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc46f309",
   "metadata": {},
   "source": [
    "# TO DO: \n",
    "\n",
    "1. Handle homographs with more than 2 pronunciations. (?)\n",
    "2. Continue to align with NK2021. \n",
    "\n",
    "## Notes:\n",
    "I only have this running on CPU right now. Struggled with getting MXNET to play well with my GPU-enabled laptop. Makes sense to do. Running this with BERT_LARGE embeddings is ridiculously slow. Model training: 159it [4h:21m:43s, 98.76s/it]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5b7da41e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import regex as re\n",
    "import csv\n",
    "import glob\n",
    "import operator\n",
    "from tqdm import tqdm\n",
    "from typing import Dict, List, Tuple\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import accuracy_score, balanced_accuracy_score\n",
    "\n",
    "# MXNET\n",
    "import mxnet as mx\n",
    "from mxnet import nd, autograd, gluon\n",
    "from mxnet.gluon import nn, Trainer\n",
    "from mxnet.gluon.data import DataLoader, ArrayDataset\n",
    "from mxnet.contrib import text\n",
    "\n",
    "# https://pypi.org/project/BERT-embedding/\n",
    "from bert_embedding import BertEmbedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "37ee2942",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PATHS\n",
    "# https://github.com/google-research-datasets/WikipediaHomographData\n",
    "TRAIN_PATH = \"./WikipediaHomographData/data/train/*.tsv\"\n",
    "TEST_PATH =  \"./WikipediaHomographData/data/eval/*.tsv\"\n",
    "WORD_IDS_PATH = \"./WikipediaHomographData/data/wordids.tsv\"\n",
    "\n",
    "# Data from Seale 2021 dissertation\n",
    "# TRAIN_PATH = \"./WHD/train_whd_fren_34_low_prev_restricted/*.tsv\"\n",
    "# TEST_PATH =  \"./WHD/test_whd_fren_34_low_prev_restricted/*.tsv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1efeb6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#HELPER FUNCTION: Used to generate global variables WORDIDS_LAB_DICT, WORDIDS_IDX_DICT\n",
    "def make_wordids_dict() -> Tuple[Dict, Dict]:\n",
    "    # FUNCTIONALITY: Makes dictionary used to convert wordids to 0,1 and vice versa\n",
    "    # OUTPUT: { homograph_str : {0: wordid_1_str, 1: wordid_2_str}, ...},\n",
    "    # { homograph_str : {wordid_1_str: 0, wordid_2_str: 1}, ...}\n",
    "    lab_dict : Dict = {}\n",
    "    idx_dict : Dict = {}\n",
    "    df : pd.DataFrame = pd.read_csv(WORD_IDS_PATH, sep=\"\\t\")\n",
    "    for hom, e in df.groupby(\"homograph\"): \n",
    "        idx = 0\n",
    "        l_dict : Dict = {}\n",
    "        i_dict : Dict = {}\n",
    "        for wid in e[\"wordid\"]:\n",
    "            i_dict[idx] = wid\n",
    "            l_dict[wid] = idx\n",
    "            idx += 1\n",
    "        lab_dict[hom] = l_dict\n",
    "        idx_dict[hom] = i_dict\n",
    "    return lab_dict, idx_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55f4f9fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WORDIDS_LAB_DICT: wordids as keys in dictionary that serves as value for homograph key\n",
      "('abstract', {'abstract_adj-nou': 0, 'abstract_vrb': 1})\n",
      "('abuse', {'abuse_nou': 0, 'abuse_vrb': 1})\n",
      "('abuses', {'abuses_nou': 0, 'abuses_vrb': 1})\n",
      "('addict', {'addict_nou': 0, 'addict_vrb': 1})\n",
      "('advocate', {'advocate_nou': 0, 'advocate_vrb': 1})\n",
      "\n",
      "\n",
      "WORDIDS_IDX_DICT: ints as keys in dictionary that serves as value for homograph key\n",
      "('abstract', {0: 'abstract_adj-nou', 1: 'abstract_vrb'})\n",
      "('abuse', {0: 'abuse_nou', 1: 'abuse_vrb'})\n",
      "('abuses', {0: 'abuses_nou', 1: 'abuses_vrb'})\n",
      "('addict', {0: 'addict_nou', 1: 'addict_vrb'})\n",
      "('advocate', {0: 'advocate_nou', 1: 'advocate_vrb'})\n"
     ]
    }
   ],
   "source": [
    "# GLOBAL VARIABLES\n",
    "# Used for cleaning tokens in get_embedding()\n",
    "REGEX = r\"(?<=[^A-Za-z])(?=[A-Za-z])|(?<=[A-Za-z])(?=[^A-Za-z])\"\n",
    "SUB = \" \"\n",
    "\n",
    "# Used to create functionality to get BERT embeddings\n",
    "BERT_SMALL = 'bert_12_768_12'\n",
    "BERT_LARGE = 'bert_24_1024_16'\n",
    "SENTENCE_LENGTH = 100 #Default sentence length is too short for some WHD sentences\n",
    "BERT_EMBEDDING = BertEmbedding(model=BERT_LARGE, max_seq_length=SENTENCE_LENGTH)\n",
    "\n",
    "# Used for training, eval\n",
    "SEED_1 = mx.random.seed(12345)\n",
    "TRAIN_DATA_SIZE = 100\n",
    "VAL_DATA_SIZE = 10\n",
    "BATCH_SIZE = 10\n",
    "EPOCHS = 10\n",
    "THRESHOLD = 0.5\n",
    "\n",
    "# Used for for label conversion\n",
    "WORDIDS_LAB_DICT, WORDIDS_IDX_DICT = make_wordids_dict()\n",
    "\n",
    "# Check out DICTS\n",
    "print(\"WORDIDS_LAB_DICT: wordids as keys in dictionary that serves as value for homograph key\")\n",
    "for e in list(WORDIDS_LAB_DICT.items())[:5]:\n",
    "    print(e)\n",
    "print(\"\\n\")\n",
    "print(\"WORDIDS_IDX_DICT: ints as keys in dictionary that serves as value for homograph key\")\n",
    "for e in list(WORDIDS_IDX_DICT.items())[:5]:\n",
    "    print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75854abb",
   "metadata": {},
   "source": [
    "## Functions "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "108804f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_embedding(sentence : str, tsv_name : str) -> List:\n",
    "    # FUNCTIONALITY: Obtain a homograph embedding from all \n",
    "    # the token embeddings of a sentence containing that homograph\n",
    "    # INPUT: \n",
    "    #    sentence: string, 1 sentence containing a homograph from tsv of sentences\n",
    "    #    csv_name: string, name of csv of training data for 1 homograph, tsv name is the homograph \n",
    "    # OUTPUT: array of float32s, the embedding of the homograph\n",
    "    \n",
    "    # Clarify that the csv name is the homograph\n",
    "    homograph = tsv_name\n",
    "    \n",
    "    # Isolate homograph tokens; separate non-alabetic characters from alphabetic ones with a space,\n",
    "    # preventing occurences like '4Minute'\n",
    "    sentence_clean = re.sub(REGEX, SUB, sentence, 0)\n",
    "    \n",
    "    # Obtain word embeddings for sentence\n",
    "    embs = BERT_EMBEDDING([sentence_clean])\n",
    "    \n",
    "    # Find homograph embedding of embeddings for each token in sentence\n",
    "    df = pd.DataFrame({'token': embs[0][0], 'embedding': embs[0][1]})\n",
    "    homograph_emb = df[df['token'] == homograph]['embedding']\n",
    "    homograph_emb = homograph_emb.tolist()\n",
    "    \n",
    "    if len(homograph_emb) < 1:\n",
    "        # Didn't find homograph in sentence, check out the problem\n",
    "        print(homograph)\n",
    "        print(sentence)\n",
    "        print(embs[0][0])\n",
    "    \n",
    "    return homograph_emb[0]\n",
    "\n",
    "def get_data(path : str) -> Tuple[List, List[str], str]:\n",
    "    # FUNCTIONALITY: Get pronunciation labels and embedding features for LR\n",
    "    # INPUT: Path to tsv with labeled sentences; 1 tsv per homograph\n",
    "    # OUTPUT: List of embedding features, list of pronunciation labels, the homograph text string\n",
    "    \n",
    "    labels : List[str] = []\n",
    "    emb_features: List = []\n",
    "    sentences : List[str] = []\n",
    "    \n",
    "    with open(path, \"r\", encoding=\"utf8\") as source: \n",
    "        for row in csv.DictReader(source, delimiter=\"\\t\"):\n",
    "            labels.append(WORDIDS_LAB_DICT[os.path.basename(path[:-4])][row[\"wordid\"]])\n",
    "            embedding = get_embedding(row['sentence'], os.path.basename(path)[:-4])\n",
    "            emb_features.append(nd.array(embedding))\n",
    "            # sentences used in debugging\n",
    "            sentences.append(row['sentence'])\n",
    "\n",
    "    labels = nd.array(labels)\n",
    "    labels = labels.astype(\"float32\")\n",
    "    \n",
    "    homograph : str = os.path.basename(path)[:-4]\n",
    "        \n",
    "    return emb_features, labels, homograph, sentences\n",
    "\n",
    "\n",
    "# Following two functions taken from: \n",
    "# https://mxnet.apache.org/versions/1.5.0/tutorials/gluon/logistic_regression_explained.html\n",
    "\n",
    "def train_model(train_dataloader):\n",
    "    cumulative_train_loss = 0\n",
    "\n",
    "    for i, (data, label) in enumerate(train_dataloader):\n",
    "        with autograd.record():\n",
    "            # Do forward pass on a batch of training data\n",
    "            output = lr_net(data)\n",
    "\n",
    "            # Calculate loss for the training data batch\n",
    "            loss_result = loss(output, label)\n",
    "\n",
    "        # Calculate gradients\n",
    "        loss_result.backward()\n",
    "\n",
    "        # Update parameters of the network\n",
    "        trainer.step(BATCH_SIZE)\n",
    "\n",
    "        # sum losses of every batch\n",
    "        cumulative_train_loss += nd.sum(loss_result).asscalar()\n",
    "\n",
    "    return cumulative_train_loss\n",
    "\n",
    "def validate_model(THRESHOLD, val_dataloader):\n",
    "    cumulative_val_loss = 0\n",
    "\n",
    "    for i, (val_data, val_ground_truth_class) in enumerate(val_dataloader):\n",
    "        # Do forward pass on a batch of validation data\n",
    "        output = lr_net(val_data)\n",
    "\n",
    "        # Similar to cumulative training loss, calculate cumulative validation loss\n",
    "        cumulative_val_loss += nd.sum(loss(output, val_ground_truth_class)).asscalar()\n",
    "\n",
    "        # Get prediction as a sigmoid\n",
    "        prediction = lr_net(val_data).sigmoid()\n",
    "\n",
    "        # Convert neuron outputs to classes\n",
    "        predicted_classes = mx.ndarray.abs(mx.nd.ceil(prediction - THRESHOLD))\n",
    "\n",
    "        # Update validation accuracy\n",
    "        accuracy.update(val_ground_truth_class, predicted_classes.reshape(-1))\n",
    "        targs_preds = (val_ground_truth_class, predicted_classes.reshape(-1))\n",
    "\n",
    "        # Calculate probabilities of belonging to different classes. F1 metric works only with this notation\n",
    "        prediction = prediction.reshape(-1)\n",
    "        probabilities = mx.nd.stack(1 - prediction, prediction, axis=1)\n",
    "\n",
    "        #f1.update(val_ground_truth_class, probabilities)\n",
    "\n",
    "    return cumulative_val_loss, targs_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdcd475",
   "metadata": {},
   "source": [
    "## Train and evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ad1d7e4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "159it [4:21:43, 98.76s/it]\n"
     ]
    }
   ],
   "source": [
    "#https://mxnet.apache.org/versions/1.5.0/tutorials/gluon/logistic_regression_explained.html\n",
    "lr_net = nn.HybridSequential()\n",
    "\n",
    "with lr_net.name_scope():\n",
    "    lr_net.add(nn.Dense(units=10, activation='relu'))\n",
    "    lr_net.add(nn.Dense(units=1)) \n",
    "\n",
    "#Hyperparameters from NK2021\n",
    "lr_net.initialize(mx.init.Xavier())\n",
    "trainer = Trainer(params=lr_net.collect_params(), optimizer='adam',\n",
    "                  optimizer_params={'learning_rate': 0.001, 'wd' : 0.01})\n",
    "\n",
    "loss = gluon.loss.SigmoidBinaryCrossEntropyLoss()\n",
    "accuracy = mx.metric.Accuracy()\n",
    "\n",
    "targ_labels = []\n",
    "pred_labels = []\n",
    "\n",
    "#Train, eval a model for each tsv\n",
    "for train_path in tqdm(glob.iglob(TRAIN_PATH)):\n",
    "    \n",
    "    features_train, targets_train, homograph, sentences = get_data(train_path)\n",
    "    train_dataset = ArrayDataset(features_train, targets_train)   \n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=False)\n",
    "\n",
    "    cum_train_loss = train_model(train_dataloader)\n",
    "        \n",
    "    test_path = train_path.replace(\"train\", \"eval\")\n",
    "    features_test, targets_test, homograph, sentences = get_data(test_path)\n",
    "    \n",
    "    val_dataset = ArrayDataset(features_test, targets_test)\n",
    "    val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "\n",
    "    for e in range(EPOCHS):\n",
    "        avg_train_loss = train_model(train_dataloader) / TRAIN_DATA_SIZE\n",
    "        cumulative_val_loss, targs_preds = validate_model(THRESHOLD, val_dataloader)\n",
    "        avg_val_loss = cumulative_val_loss / VAL_DATA_SIZE\n",
    "        \n",
    "        hom_dict = WORDIDS_IDX_DICT[homograph]\n",
    "        try:\n",
    "            targ_labels.extend(hom_dict[int(i.asscalar())] for i in targs_preds[0])\n",
    "            pred_labels.extend(hom_dict[int(i.asscalar())] for i in targs_preds[1])\n",
    "        except:\n",
    "            print(hom_dict)\n",
    "            print(targs_preds)\n",
    "        accuracy.reset()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05be0862",
   "metadata": {},
   "source": [
    "## Metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "916c02e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy\n",
      "0.9831309904153355\n",
      "Balanced accuracy\n",
      "0.9575439891718961\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jen/opt/anaconda3/lib/python3.7/site-packages/sklearn/metrics/_classification.py:1953: UserWarning: y_pred contains classes not in y_true\n",
      "  warnings.warn(\"y_pred contains classes not in y_true\")\n"
     ]
    }
   ],
   "source": [
    "print(\"Accuracy\")\n",
    "print(accuracy_score(targ_labels, pred_labels))\n",
    "print(\"Balanced accuracy\")\n",
    "print(balanced_accuracy_score(targ_labels, pred_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25dafbde",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
